---
id: "1242"
title: "Web Crawler Multithreaded"
companyTags: "Unknown"
slug: "web-crawler-multithreaded"
difficulty: "Medium"
tags: ["Depth-First Search", "Breadth-First Search", "Concurrency"]
---

### Explanation:

This problem involves implementing a multithreaded web crawler, where multiple threads work together to crawl a given set of URLs. The main idea is to have a shared queue of URLs to be crawled and multiple threads that fetch a URL from the queue, crawl it, extract more URLs from the crawled page, and add them back to the queue for processing by other threads.

Algorithm:
1. Create a shared queue to store URLs to be crawled.
2. Initialize a set to keep track of visited URLs to avoid duplicates.
3. Implement a function to fetch a URL, crawl it, extract more URLs, and add them to the queue and visited set.
4. Create multiple threads that continuously fetch URLs from the queue, crawl them, and repeat the process until all URLs are crawled.
5. Ensure thread safety by using synchronization mechanisms like locks to access the shared queue and visited set.

Time Complexity:
- The time complexity of this algorithm depends on the number of URLs to be crawled and the efficiency of the crawling process. Let's denote `N` as the number of URLs.
- In the worst-case scenario, each URL might lead to extracting multiple new URLs, resulting in a total of `O(N)` URLs to be crawled.
- The time complexity can be `O(N)` in the worst case.

Space Complexity:
- The space complexity is also influenced by the number of URLs to be crawled and the storage required for the shared queue and visited set.
- The space complexity can be `O(N)` in the worst case.

:

```java
// Java Solution
import java.util.concurrent.*;
import java.util.HashSet;
import java.util.Set;

class WebCrawlerMultithreaded {
    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        Set<String> visited = new HashSet<>();
        visited.add(startUrl);
        
        BlockingQueue<String> queue = new LinkedBlockingQueue<>();
        queue.add(startUrl);
        
        int numOfThreads = 4; // Number of threads to use
        
        ExecutorService executor = Executors.newFixedThreadPool(numOfThreads);
        
        for (int i = 0; i < numOfThreads; i++) {
            executor.execute(() -> {
                while (true) {
                    String url = queue.poll();
                    if (url == null) break;
                    
                    for (String nextUrl : htmlParser.getUrls(url)) {
                        if (!visited.contains(nextUrl)) {
                            visited.add(nextUrl);
                            queue.add(nextUrl);
                        }
                    }
                }
            });
        }
        
        executor.shutdown();
        try {
            executor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        
        return new ArrayList<>(visited);
    }
}
```

```cpp
// C++ Solution
#include <queue>
#include <unordered_set>
#include <mutex>
#include <thread>
#include <condition_variable>

class WebCrawlerMultithreaded {
public:
    std::vector<std::string> crawl(std::string startUrl, HtmlParser htmlParser) {
        std::unordered_set<std::string> visited;
        visited.insert(startUrl);
        
        std::queue<std::string> urlsQueue;
        urlsQueue.push(startUrl);
        
        int numOfThreads = 4; // Number of threads to use
        std::mutex mtx;
        std::condition_variable cv;
        bool finished = false;
        
        auto crawlerFunc = [&]() {
            while (true) {
                std::string url;
                {
                    std::unique_lock<std::mutex> lock(mtx);
                    cv.wait(lock, [&]{ return !urlsQueue.empty() || finished; });
                    if (urlsQueue.empty() && finished) break;
                    url = urlsQueue.front();
                    urlsQueue.pop();
                }
                
                for (std::string nextUrl : htmlParser.getUrls(url)) {
                    std::lock_guard<std::mutex> lock(mtx);
                    if (visited.find(nextUrl) == visited.end()) {
                        visited.insert(nextUrl);
                        urlsQueue.push(nextUrl);
                    }
                }
            }
        };
        
        std::vector<std::thread> threads;
        for (int i = 0; i < numOfThreads; i++) {
            threads.emplace_back(crawlerFunc);
        }
        
        {
            std::lock_guard<std::mutex> lock(mtx);
            finished = true;
        }
        cv.notify_all();
        
        for (std::thread& t : threads) {
            t.join();
        }
        
        return std::vector<std::string>(visited.begin(), visited.end());
    }
};
```

```python
# Python Solution
from queue import Queue
from threading import Thread

class WebCrawlerMultithreaded:
    def crawl(self, startUrl: str, htmlParser: 'HtmlParser') -> List[str]:
        visited = set()
        visited.add(startUrl)
        
        queue = Queue()
        queue.put(startUrl)
        
        num_of_threads = 4  # Number of threads to use
        
        def crawler_func():
            while True:
                url = queue.get()
                if url is None:
                    break
                
                for next_url in htmlParser.getUrls(url):
                    if next_url not in visited:
                        visited.add(next_url)
                        queue.put(next_url)
        
        threads = []
        for _ in range(num_of_threads):
            thread = Thread(target=crawler_func)
            thread.start()
            threads.append(thread)
        
        for thread in threads:
            thread.join()
        
        return list(visited)
```